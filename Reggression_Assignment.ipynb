{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. Simple Linear Regression vs Multiple Linear Regression\n",
        "# Simple Linear Regression: Predicts a dependent variable using a single independent variable.\n",
        "# Multiple Linear Regression: Predicts a dependent variable using two or more independent variables.\n",
        "\n",
        "# Example of Simple Linear Regression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Simple Linear Regression Example\n",
        "# Independent variable (X) and Dependent variable (y)\n",
        "X = np.array([[1], [2], [3], [4], [5]])  # feature (e.g., years of experience)\n",
        "y = np.array([1, 2, 3, 4, 5])            # target variable (e.g., salary)\n",
        "\n",
        "# Create a simple linear regression model and fit it\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plotting the data points and the regression line\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.xlabel('Independent Variable (X)')\n",
        "plt.ylabel('Dependent Variable (y)')\n",
        "plt.show()\n",
        "\n",
        "# Example of Multiple Linear Regression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generating a dataset for multiple regression (2 features)\n",
        "X_multi, y_multi = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
        "\n",
        "# Fit a multiple linear regression model\n",
        "model_multi = LinearRegression()\n",
        "model_multi.fit(X_multi, y_multi)\n",
        "\n",
        "# Predictions\n",
        "y_multi_pred = model_multi.predict(X_multi)\n",
        "\n",
        "# Q2. Assumptions of Linear Regression:\n",
        "# 1. Linearity: The relationship between independent and dependent variables should be linear.\n",
        "# 2. Independence: Observations should be independent of each other.\n",
        "# 3. Homoscedasticity: Constant variance of errors.\n",
        "# 4. Normality: Residuals should be normally distributed.\n",
        "\n",
        "# Check assumptions\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "\n",
        "# Fit a linear model\n",
        "X_with_const = sm.add_constant(X)\n",
        "model_stats = sm.OLS(y, X_with_const).fit()\n",
        "\n",
        "# Check for residuals' normality using histogram\n",
        "residuals = model_stats.resid\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Residuals Normality')\n",
        "plt.show()\n",
        "\n",
        "# Q3. Slope and Intercept Interpretation\n",
        "# The slope represents the change in the dependent variable for each unit change in the independent variable.\n",
        "# The intercept is the expected value of the dependent variable when the independent variable is zero.\n",
        "\n",
        "# For a simple linear regression model y = mx + c:\n",
        "# m = slope, c = intercept\n",
        "# Example: y = 2x + 1\n",
        "# If x = 3, y = 2*3 + 1 = 7 (The slope tells us that for each increase of 1 in x, y increases by 2)\n",
        "\n",
        "# Q4. Gradient Descent\n",
        "# Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.\n",
        "\n",
        "# Example: For linear regression, we use gradient descent to minimize the Mean Squared Error (MSE) to find the optimal coefficients.\n",
        "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
        "    m, c = 0, 0  # initial guess for slope and intercept\n",
        "    n = len(X)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        y_pred = m * X + c\n",
        "        cost = (1/n) * sum((y_pred - y) ** 2)\n",
        "\n",
        "        # Calculate gradients\n",
        "        m_gradient = (2/n) * sum((y_pred - y) * X)\n",
        "        c_gradient = (2/n) * sum(y_pred - y)\n",
        "\n",
        "        # Update parameters\n",
        "        m -= learning_rate * m_gradient\n",
        "        c -= learning_rate * c_gradient\n",
        "\n",
        "    return m, c\n",
        "\n",
        "# Using gradient descent to optimize for m and c\n",
        "X_gradient = np.array([1, 2, 3, 4, 5])\n",
        "y_gradient = np.array([1, 2, 3, 4, 5])\n",
        "m, c = gradient_descent(X_gradient, y_gradient)\n",
        "print(f\"Optimized m (slope): {m}, c (intercept): {c}\")\n",
        "\n",
        "# Q5. Multiple Linear Regression Model\n",
        "# Multiple linear regression predicts a dependent variable using more than one independent variable.\n",
        "# The model is: y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n",
        "# This is a generalization of simple linear regression, where more than one feature is involved.\n",
        "\n",
        "# Q6. Multicollinearity in Multiple Linear Regression\n",
        "# Multicollinearity occurs when independent variables in the model are highly correlated with each other.\n",
        "# This can make the model's coefficients unstable and difficult to interpret.\n",
        "\n",
        "# Detecting multicollinearity using VIF (Variance Inflation Factor)\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "X_multi_with_const = sm.add_constant(X_multi)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = range(X_multi_with_const.shape[1])\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X_multi_with_const.values, i) for i in range(X_multi_with_const.shape[1])]\n",
        "print(vif)\n",
        "\n",
        "# Addressing multicollinearity: Drop one of the correlated features or apply dimensionality reduction techniques like PCA.\n",
        "\n",
        "# Q7. Polynomial Regression\n",
        "# Polynomial regression fits a nonlinear relationship between the independent variable(s) and the dependent variable.\n",
        "# It's an extension of linear regression, but uses polynomial terms of the independent variables.\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model_poly = LinearRegression()\n",
        "model_poly.fit(X_poly, y)\n",
        "\n",
        "# Q8. Advantages and Disadvantages of Polynomial Regression\n",
        "# Advantages:\n",
        "# - Can model nonlinear relationships.\n",
        "# - Can capture more complex patterns in the data.\n",
        "\n",
        "# Disadvantages:\n",
        "# - Overfitting: If the degree of the polynomial is too high, the model can fit noise in the data.\n",
        "# - More computationally expensive.\n",
        "\n",
        "# When to use polynomial regression:\n",
        "# - When there is a known nonlinear relationship between variables and simple linear regression is insufficient.\n",
        "\n",
        "# Example use case: Predicting house prices based on square footage, where the relationship between price and size is nonlinear.\n"
      ]
    }
  ]
}